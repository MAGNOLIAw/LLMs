{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install"
      ],
      "metadata": {
        "id": "Vfw9RTQsg1KP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXYUuhc5ly10",
        "outputId": "5ddab993-250a-4d1a-c63f-0cab465935a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.2.1+cu121 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.2.1+cu121\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 安装 PyTorch\n",
        "!pip install torch==2.2.1+cu121"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Attention"
      ],
      "metadata": {
        "id": "5MjA-6mBg_2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "bSU26VPFojkT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_seq = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n",
        "input_seq = torch.randn(3, 5)\n",
        "print(input_seq.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h6KWV0mpKa7",
        "outputId": "6736fa7a-d510-434c-8210-c333ac2be848"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成 Key、Query 和 Value 矩阵的随机权重\n",
        "l, d_in = input_seq.shape\n",
        "d_model = 8\n",
        "weight_q = torch.randn(d_in, d_model)\n",
        "weight_k = torch.randn(d_in, d_model)\n",
        "weight_v = torch.randn(d_in, d_model)\n",
        "\n",
        "query = torch.matmul(input_seq, weight_q)\n",
        "key = torch.matmul(input_seq, weight_k)\n",
        "value = torch.matmul(input_seq, weight_v)\n",
        "\n",
        "# #### Scaled dot-product attention\n",
        "# (batch_size, num_heads, query_len, key_len)\n",
        "att_scores = torch.matmul(query, key.T) / d_model**0.5\n",
        "att_scores = F.softmax(att_scores, dim=-1)\n",
        "\n",
        "output = torch.matmul(att_scores, value)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1mVF61VphSd",
        "outputId": "107aed3d-f5af-4f8c-e85c-008549a7432d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tWYuDf3LtDZV"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define model"
      ],
      "metadata": {
        "id": "yhGVkOlOtC2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "3R5Hjfjjqe6V"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Embeddings"
      ],
      "metadata": {
        "id": "qm7-lmGMoxhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Why `max_len`?\n",
        "    - x.size(1) 是当前 batch 的实际序列长度。\n",
        "    - 从 self.pe 中截取前 x.size(1) 部分即可。\n",
        "    - 这样不用每次重新算，只需要切片即可。\n",
        "- `torch.arange(0, d_model, 2)` 就是取出所有偶数维度索引"
      ],
      "metadata": {
        "id": "gHv_8R5tV1Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1)\n",
        "\n",
        "        div_term = torch.exp(-torch.arange(0, d_model, 2).float() * torch.log(torch.tensor(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0) # (1, max_len, d_model)\n",
        "\n",
        "        self.register_buffer('pe', pe)  # not a parameter, but part of the module\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 为什么使用x.size(1)？因为PE可以无限长\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "xvOI3aZatUZE"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 示例用法\n",
        "d_model = 512\n",
        "max_len = 100\n",
        "num_heads = 8\n",
        "\n",
        "# 位置编码\n",
        "pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "# 示例输入序列\n",
        "input_sequence = torch.randn(5, max_len, d_model)\n",
        "\n",
        "# 应用位置编码\n",
        "input_sequence = pos_encoder(input_sequence)\n",
        "print(\"输入序列的位置编码:\")\n",
        "print(input_sequence.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDbXMl-VuMqj",
        "outputId": "81171fbd-3887-4cfc-8b98-735e63dde333"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "输入序列的位置编码:\n",
            "torch.Size([5, 100, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-attention: scaled dot product"
      ],
      "metadata": {
        "id": "ykgkbqvmo75j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 注意力 (Attention) 的计算方式\n",
        "在 Transformer 里，**自注意力 (self-attention)** 的计算核心是：\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + \\text{mask}\\right)V\n",
        "$$\n",
        "\n",
        "- `Q` (queries)：每个位置要去“提问”  \n",
        "- `K` (keys)：每个位置的“特征”  \n",
        "- `QK^T`：得到一个 **[seq_len × seq_len] 的矩阵**，表示每个位置对所有位置的“相关性分数”。\n",
        "\n",
        "2. mask为什么设置为-inf?\n",
        "- 将被屏蔽位置的分数设为 -1e9 或 -inf 是为了确保在 softmax 操作后，这些位置的注意力权重为零，从而实现真正的屏蔽效果。直接将分数设为 0 并不能完全屏蔽这些位置，可能会导致模型在训练或推理时关注到不应关注的部分。"
      ],
      "metadata": {
        "id": "NWL--yy5ZXTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def att(query, key, value, mask=None, dropout=None):\n",
        "    d_k = query.size(-1) # feature dimension\n",
        "    scores = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "\n",
        "    # if masked or not\n",
        "    # mask：布尔类型的掩码张量，形状需要与原始张量相同或可广播。value：用于填充的值，当 mask 中对应位置为 True 时，原始张量中的该位置将被替换为 value\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask==0, -1e9)\n",
        "\n",
        "    att_scores = F.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        att_scores = dropout(att_scores)\n",
        "\n",
        "    output = torch.matmul(att_scores, value)\n",
        "    print('output_att:', output.shape)\n",
        "    return output, att_scores"
      ],
      "metadata": {
        "id": "7ugyhVpN3nRq"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-head attention"
      ],
      "metadata": {
        "id": "oo4UKwTTpcCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "将输入张量 x 的形状从 [batch_size, seq_len, d_model] 重塑为 [batch_size, num_heads, seq_len, depth]"
      ],
      "metadata": {
        "id": "0H3SrJkqVF5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_head(self, x):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        x = x.view(batch_size, seq_len, num_heads, self.depth).transpose(1, 2)\n",
        "        # 将输入张量 x 的形状从 [batch_size, seq_len, d_model] 重塑为 [batch_size, num_heads, seq_len, depth]\n",
        "        # 将嵌入维度 d_model 分解为多个注意力头。\n",
        "        return x\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        query = self.w_q(query)\n",
        "        key = self.w_k(key)\n",
        "        value = self.w_v(value)\n",
        "\n",
        "        # split heads\n",
        "        query = self.split_head(query)\n",
        "        key = self.split_head(key)\n",
        "        value = self.split_head(value)\n",
        "\n",
        "        # scaled dot product attention\n",
        "        x, att_scores = att(query, key, value, mask, self.dropout)\n",
        "\n",
        "        batch_size, _, seq_len, depth = x.size()\n",
        "        # 换回原来的顺序（从 [batch, heads, seq, depth] → [batch, seq, heads, depth]）\n",
        "        # 把最后两维 flatten 成 d_model = heads × depth，得到最终形状 [batch_size, seq_length, d_model]\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        x = self.w_o(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4ehsg-I7AZ_X"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 示例用法\n",
        "d_model = 512\n",
        "max_len = 100\n",
        "num_heads = 8\n",
        "\n",
        "# 多头注意力\n",
        "multihead_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "# 示例输入序列\n",
        "input_sequence = torch.randn(5, max_len, d_model)\n",
        "\n",
        "att_output = multihead_attn(input_sequence, input_sequence, input_sequence)\n",
        "print(\"output_MH_att:\", att_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2YaCLAIk6L2",
        "outputId": "a52a9183-b341-405b-ed58-79cd8d1d0861"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output_att: torch.Size([5, 8, 100, 64])\n",
            "output_MH_att: torch.Size([5, 100, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed forward layer"
      ],
      "metadata": {
        "id": "QsjE8a8MqIZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 前馈网络的代码实现\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 线性变换1\n",
        "        x = self.relu(self.linear1(x))\n",
        "\n",
        "        # 线性变换2\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Vssfr5Crn4f7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example"
      ],
      "metadata": {
        "id": "J7I7cYL6qeoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 示例用法\n",
        "d_model = 512\n",
        "max_len = 100\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "\n",
        "# 多头注意力\n",
        "multihead_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "# 前馈网络\n",
        "ff_network = FeedForward(d_model, d_ff)\n",
        "\n",
        "# 示例输入序列\n",
        "input_sequence = torch.randn(5, max_len, d_model)\n",
        "\n",
        "# 多头注意力\n",
        "attention_output= multihead_attn(input_sequence, input_sequence, input_sequence)\n",
        "\n",
        "# 前馈网络\n",
        "output_ff = ff_network(attention_output)\n",
        "print('input_sequence',input_sequence.shape)\n",
        "print(\"output_ff\", output_ff.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hffcOHFmoCNO",
        "outputId": "c5c2d1a6-6d05-44f4-8ae3-2663fbaaecb9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output_att: torch.Size([5, 8, 100, 64])\n",
            "input_sequence torch.Size([5, 100, 512])\n",
            "output_ff torch.Size([5, 100, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "ELgmye3oqh5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0nkmLhiAWIfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=None):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.multihead_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        residual = x\n",
        "\n",
        "        # ### Multi-head attention\n",
        "        att_outputs = self.multihead_attn(x, x, x, mask)\n",
        "\n",
        "        # Add & Norm\n",
        "        x = x + att_outputs\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # ### Feed Forward\n",
        "        ff_outputs = self.feed_forward(x)\n",
        "\n",
        "        # Add & Norm\n",
        "        x = x + ff_outputs\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "DjSsSLNzo6dI"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "max_len = 100\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "\n",
        "encoder_layer = EncoderLayer(d_model, num_heads, d_ff, 0.1)\n",
        "\n",
        "input_sequence = torch.randn(5, max_len, d_model)\n",
        "\n",
        "encoder_output = encoder_layer(input_sequence)\n",
        "print(\"encoder output shape:\", encoder_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnfiUhRHpzR4",
        "outputId": "fa7e6c1c-a121-48cd-babe-fdcd24d32d18"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output_att: torch.Size([5, 8, 100, 64])\n",
            "encoder output shape: torch.Size([5, 100, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "XRrVhmt9qmRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 解码器的代码实现\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.masked_self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.enc_dec_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "\n",
        "        # ### masked multi-head self attention\n",
        "        # 掩码的自注意力层\n",
        "        self_attention_output= self.masked_self_attention(x, x, x, tgt_mask)\n",
        "        self_attention_output = self.dropout(self_attention_output)\n",
        "        # add & norm\n",
        "        x = x + self_attention_output\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # ### multi-head cross attention\n",
        "        # 编码器-解码器注意力层\n",
        "        enc_dec_attention_output= self.enc_dec_attention(x, encoder_output,\n",
        "        encoder_output, src_mask) # q, k, v, mask\n",
        "        # add & norm\n",
        "        enc_dec_attention_output = self.dropout(enc_dec_attention_output)\n",
        "        x = x + enc_dec_attention_output\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        # ## feed forward\n",
        "        # 前馈层\n",
        "        feed_forward_output = self.feed_forward(x)\n",
        "        feed_forward_output = self.dropout(feed_forward_output)\n",
        "        # add & norm\n",
        "        x = x + feed_forward_output\n",
        "        x = self.norm3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "y4imv8bjuUTV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义DecoderLayer的参数\n",
        "d_model = 512  # 模型的维度\n",
        "num_heads = 8  # 注意力头的数量\n",
        "d_ff = 2048    # 前馈网络的维度\n",
        "dropout = 0.1  # 丢弃概率\n",
        "\n",
        "batch_size = 1 # 批量大小\n",
        "max_len = 100  # 序列的最大长度\n",
        "\n",
        "# 定义DecoderLayer实例\n",
        "decoder_layer = DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "\n",
        "src_mask = torch.rand(batch_size, max_len, max_len) > 0.5\n",
        "tgt_mask = torch.tril(torch.ones(max_len, max_len)).unsqueeze(0) == 0\n",
        "\n",
        "# 将输入张量传递到DecoderLayer\n",
        "output = decoder_layer(input_sequence, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "# 输出形状\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt6mRIQsvSTj",
        "outputId": "9e9bf1eb-1d6f-49de-8c7a-ff4360ba1ddc"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output_att: torch.Size([5, 8, 100, 64])\n",
            "output_att: torch.Size([5, 8, 100, 64])\n",
            "Output shape: torch.Size([5, 100, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "v1K5GyLCrf1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFORMER的实现\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff,\n",
        "    max_len, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        # ### embedding\n",
        "        # 定义编码器和解码器的词嵌入层\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        # ### positional embedding\n",
        "        # 定义位置编码层\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # 定义编码器和解码器的多层堆叠\n",
        "        # ### encoders\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "        for _ in range(num_layers)])\n",
        "        # ### decoders\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "        for _ in range(num_layers)])\n",
        "\n",
        "        # 定义线性层\n",
        "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # 生成掩码\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2) # 最后形状会变成 [batch_size, 1, 1, src_len]\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3) # 形状变成 [batch_size, 1, tgt_len, 1]\n",
        "\n",
        "        seq_length = tgt.size(1)\n",
        "\n",
        "        # 取上三角（不含对角线），得到对角线上方全是 1，其余是 0。\n",
        "        # `1-...` 翻转，使得上三角变成 0，对角线及下三角变成 1。\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    # 前向传播\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "\n",
        "        # 编码器输入的词嵌入和位置编码\n",
        "        encoder_embedding = self.encoder_embedding(src)\n",
        "        en_positional_encoding = self.positional_encoding(encoder_embedding)\n",
        "        src_embedded = self.dropout(en_positional_encoding)\n",
        "\n",
        "        # 解码器输入的词嵌入和位置编码\n",
        "        decoder_embedding = self.decoder_embedding(tgt)\n",
        "        de_positional_encoding = self.positional_encoding(decoder_embedding)\n",
        "        tgt_embedded = self.dropout(de_positional_encoding)\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            print('[ENC]')\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            print('[DEC]')\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.linear(dec_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "S7BXaC1Jvl_V"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 为什么是 0 ~ vocab_size-1\n",
        "    - 我们会给词表（vocabulary）里的每一个 token 分配一个唯一的 整数 ID。"
      ],
      "metadata": {
        "id": "E8z5DsKLh_G0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 示例用法\n",
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "\n",
        "max_len = 100\n",
        "dropout = 0.1\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout)\n",
        "\n",
        "# 生成随机示例数据\n",
        "src_data = torch.randint(1, src_vocab_size, (5, max_len))  # size 是 (batch_size, seq_length), “一次要看多少句话”\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (5, max_len))  # size 是 (batch_size, seq_length)\n",
        "\n",
        "# 如果原始 tgt_data 的长度是 100（含 <bos> 和 <eos>），\n",
        "# 模型的 decoder 输入 是 tgt_data[:, :-1]（从 <bos> 开始，到最后一个 token 的前一个）。\n",
        "# 模型的 预测目标 是 tgt_data[:, 1:]（从第一个实际词开始，到 <eos>）。\n",
        "transformer(src_data, tgt_data[:, :-1]).shape   # (batch_size, seq_length, tgt_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMQGXn07v9EI",
        "outputId": "e5463fb6-927d-4fae-f760-58bfe04b0b53"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ENC]\n",
            "output_att: torch.Size([5, 8, 100, 64])\n",
            "[ENC]\n",
            "output_att: torch.Size([5, 8, 100, 64])\n",
            "[ENC]\n",
            "output_att: torch.Size([5, 8, 100, 64])\n",
            "[ENC]\n",
            "output_att: torch.Size([5, 8, 100, 64])\n",
            "[ENC]\n",
            "output_att: torch.Size([5, 8, 100, 64])\n",
            "[ENC]\n",
            "output_att: torch.Size([5, 8, 100, 64])\n",
            "[DEC]\n",
            "output_att: torch.Size([5, 8, 99, 64])\n",
            "output_att: torch.Size([5, 8, 99, 64])\n",
            "[DEC]\n",
            "output_att: torch.Size([5, 8, 99, 64])\n",
            "output_att: torch.Size([5, 8, 99, 64])\n",
            "[DEC]\n",
            "output_att: torch.Size([5, 8, 99, 64])\n",
            "output_att: torch.Size([5, 8, 99, 64])\n",
            "[DEC]\n",
            "output_att: torch.Size([5, 8, 99, 64])\n",
            "output_att: torch.Size([5, 8, 99, 64])\n",
            "[DEC]\n",
            "output_att: torch.Size([5, 8, 99, 64])\n",
            "output_att: torch.Size([5, 8, 99, 64])\n",
            "[DEC]\n",
            "output_att: torch.Size([5, 8, 99, 64])\n",
            "output_att: torch.Size([5, 8, 99, 64])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 99, 5000])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ]
}